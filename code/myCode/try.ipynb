{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w=torch.tensor([2,3.0])\n",
    "true_b=4\n",
    "features,labels=d2l.synthetic_data(true_w,true_b,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_array,batch_size,is_train=True):\n",
    "    data_set=data.TensorDataset(*data_array)\n",
    "    return data.DataLoader(data_set,batch_size,shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "data_iter=load_array((features,labels),batch_size,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=torch.optim.SGD(net.parameters(),0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 loss:0.000182607356691733\n",
      "epoch:2 loss:0.00010383463086327538\n",
      "epoch:3 loss:0.00010344846668886021\n",
      "epoch:4 loss:0.00010374355042586103\n",
      "epoch:5 loss:0.00010261215356877074\n",
      "epoch:6 loss:0.00010260509588988498\n",
      "epoch:7 loss:0.00010286425822414458\n",
      "epoch:8 loss:0.00010309965728083625\n",
      "epoch:9 loss:0.00010375152487540618\n",
      "epoch:10 loss:0.00010319631110178307\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter:\n",
    "        y_hat=net(X)\n",
    "        l=loss(y_hat,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        \n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch:{epoch+1} loss:{l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for idx,module in enumerate(args):\n",
    "            self._modules[str(idx)]=module\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "            \n",
    "        return X\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross(X,K):\n",
    "    h,w=K.shape\n",
    "    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i,h,w=X.shape\n",
    "    c_o=K.shape[0]\n",
    "    X.reshape(c_i,h*w)\n",
    "    K.reshape(c_o,c_i)\n",
    "    Y=torch.matmul(K,X)\n",
    "    Y.reshape(c_o,h,w)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 21 08:32:31 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 532.09                 Driver Version: 532.09       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 L...  WDDM | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P8                9W /  N/A|      0MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "print(device_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n",
      "tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.zeros(1).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    layers=[]\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels=out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch=((1,64),(1,128),(2,256),(2,512),(2,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks=[]\n",
    "    in_channels=1\n",
    "    \n",
    "    for (num_convs,out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))\n",
    "        in_channels=out_channels\n",
    "        \n",
    "    return nn.Sequential(*conv_blks,nn.Flatten(),\n",
    "                         nn.Linear(out_channels*7*7,4096),nn.ReLU(),nn.Dropout(0.5),\n",
    "                         nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(0.5),\n",
    "                         nn.Linear(4096,10))\n",
    "    \n",
    "net=vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential shape\t torch.Size([1, 64, 112, 112])\n",
      "Sequential shape\t torch.Size([1, 128, 56, 56])\n",
      "Sequential shape\t torch.Size([1, 256, 28, 28])\n",
      "Sequential shape\t torch.Size([1, 512, 14, 14])\n",
      "Sequential shape\t torch.Size([1, 512, 7, 7])\n",
      "Flatten shape\t torch.Size([1, 25088])\n",
      "Linear shape\t torch.Size([1, 4096])\n",
      "ReLU shape\t torch.Size([1, 4096])\n",
      "Dropout shape\t torch.Size([1, 4096])\n",
      "Linear shape\t torch.Size([1, 4096])\n",
      "ReLU shape\t torch.Size([1, 4096])\n",
      "Dropout shape\t torch.Size([1, 4096])\n",
      "Linear shape\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'shape\\t', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def nin_block(in_channels,out_channels,kernel_size,strides,padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),nn.ReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __inti__(self,in_channels,c1,c2,c3,c4,**kwargs):\n",
    "        super(Inception,self).__init__(**kwargs)\n",
    "        \n",
    "        self.p1_1=nn.Conv2d(in_channels,c1,kernel_size=1)\n",
    "        \n",
    "        self.p2_1=nn.Conv2d(in_channels,c2[0],kernel_size=1)\n",
    "        self.p2_2=nn.Conv2d(c2[0],c2[1],kernel_size=3,padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        \n",
    "        return torch.cat((p1,p2,p3,p4),dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not torch.is_grad_enabled():\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape)in(2,4)\n",
    "\n",
    "        if len(X.shape)==2:\n",
    "            mean=X.mean(dim=0)\n",
    "            var=((X-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            mean=X.mean(dim=(0,2,3),keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=(0,2,3),keepdim=True)\n",
    "        X_hat=(X-mean)/torch.sqrt(var+eps)\n",
    "\n",
    "        moving_mean=momentum*moving_mean+(1.0-momentum)*mean\n",
    "        moving_var=momentum*moving_var+(1.0-momentum)*var\n",
    "    Y=gamma*X_hat+beta\n",
    "    return Y,moving_mean.data,moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self,input_channels,num_channels,use_1x1conv=False,strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(input_channels,num_channels,kernel_size=3,padding=1,strides=strides)\n",
    "        self.conv2=nn.Conv2d(num_channels,num_channels,kernel_size=3,padding=1)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv3=nn.Conv2d(input_channels,num_channels,kernel_size=3,stride=strides)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "        \n",
    "        self.bn1=nn.BatchNorm2d(num_channels)\n",
    "        self.bn2=nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self,X):\n",
    "        Y=F.relu(self.bn1(self.conv1(X)))\n",
    "        Y=self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        Y+=X\n",
    "\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=nn.Sequential(nn.Conv2d(1,64,7,2,3),\n",
    "                 nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                 nn.MaxPool2d(kernel_size=3,stride=2,padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels,num_channels,num_residuals,first_block=False):\n",
    "    blk=[]\n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not first_block:\n",
    "            blk.append(Residual(input_channels,num_residuals,use_1x1conv=True,strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels,num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=nn.Sequential(*resnet_block(64,64,2,first_block=True))\n",
    "b3=nn.Sequential(*resnet_block(64,128,2))\n",
    "b4=nn.Sequential(*resnet_block(128,256,2))\n",
    "b5=nn.Sequential(*resnet_block(256,512,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(b1,b2,b3,b4,b5,\n",
    "                  nn.AdaptiveMaxPool2d((1,1)),\n",
    "                  nn.Flatten(),nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1():\n",
    "    a=1\n",
    "    b=2\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=fun1()\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
