{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分数：0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import copy\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "import chardet\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "from dgl.dataloading import BlockSampler, NodeDataLoader, MultiLayerFullNeighborSampler\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from dgl.nn import SAGEConv,GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 换cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查CUDA是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    df_data = pd.read_csv(os.path.join(data_path), encoding=encoding)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(ifLoadData, graphPath, dataNodePath, dataEdgePath,isTrain):\n",
    "    if(ifLoadData):\n",
    "        with open(graphPath, 'rb') as file:\n",
    "            graph_list = pickle.load(file)\n",
    "    else:\n",
    "        graph_list = []\n",
    "        node_feats = csv_reader(dataNodePath)\n",
    "        edge_feats = csv_reader(dataEdgePath)\n",
    "        \n",
    "        for c_name, c_df in tqdm(node_feats.groupby('date_id'), desc='Building Graphs'):\n",
    "            date_src, date_tgt, date_edge_feat = [], [], []\n",
    "            date_edge = edge_feats[edge_feats['date_id'] == c_name]\n",
    "            # 在一天的数据里遍历所有的点，建边\n",
    "            # c_name是分组的日期值，c_df是组内的数据（包含日期）\n",
    "            for i in tqdm(range(len(c_df)), desc='Date:{}'.format(c_name)):\n",
    "                now_node = c_df.iloc[i] # 这个日期里的第i个数据，有所有种类的数据\n",
    "                src = now_node['geohash_id'] # 这行数据是属于哪个点的\n",
    "                edge_feat = date_edge[date_edge['geohash6_point1'] == src] # 与这个点有关的所有的边\n",
    "                # 是有向边\n",
    "                tgt = edge_feat['geohash6_point2'] # 有关边的target点\n",
    "                date_src.extend([src for _ in range(len(tgt))]) # 产生对应数量的边的src\n",
    "                date_tgt.extend(tgt) # 加入对应的tgt\n",
    "                date_edge_feat.extend(pd.concat([edge_feat['F_1'], edge_feat['F_2']])) # 添加边的特征\n",
    "            date_src = np.array(date_src)\n",
    "            date_tgt = np.array(date_tgt)\n",
    "            now_graph = dgl.graph((np.concatenate([np.array(date_src), np.array(date_tgt)], axis=0), np.concatenate([np.array(date_tgt), np.array(date_src)], axis=0)))\n",
    "\n",
    "            scaler = StandardScaler() \n",
    "\n",
    "            if isTrain:\n",
    "                nodes_data = c_df.drop(['geohash_id', 'date_id', 'active_index', 'consume_index'], axis=1)\n",
    "            else:\n",
    "                nodes_data = c_df.drop(['geohash_id', 'date_id'], axis=1)\n",
    "            \n",
    "            now_graph.edata['feat'] = torch.from_numpy(scaler.fit_transform(np.concatenate([np.array(date_edge_feat), np.array(date_edge_feat)], axis=0).reshape(-1,2))).float()\n",
    "            \n",
    "            now_graph.ndata['feat'] = torch.from_numpy(scaler.fit_transform(nodes_data.values)).float()\n",
    "\n",
    "            if isTrain:\n",
    "                now_graph.ndata['active'] = torch.from_numpy(c_df['active_index'].values).float()\n",
    "                \n",
    "                now_graph.ndata['consume'] = torch.from_numpy(c_df['consume_index'].values).float()\n",
    "\n",
    "            graph_list.append(now_graph)\n",
    "\n",
    "        # # 全部处理完后划分数据集\n",
    "        # index = list(range(graph_list[0].number_of_nodes()))\n",
    "        # train_idx, test_idx = train_test_split(index, test_size=args['testSize'], random_state=2, shuffle=True)\n",
    "\n",
    "        with open(graphPath, 'wb') as file:\n",
    "            pickle.dump(graph_list, file)\n",
    "\n",
    "    # return graph_list, train_idx, test_idx\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifLoadData=True\n",
    "graphPath=\"../data/graphs.pkl\"\n",
    "dataNodePath=\"../data/pre_train_90.csv\"\n",
    "dataEdgePath=\"../data/pre_edge_90.csv\"\n",
    "isTrain=True\n",
    "\n",
    "graph_list  = build_graph(ifLoadData, graphPath, dataNodePath, dataEdgePath,isTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [graph.to(device) for graph in graph_list] # 按天将图图取出来，就是src和tgt\n",
    "features = [graph.ndata['feat'].to(device) for graph in graph_list] # 每天(图)点的特征\n",
    "edge_features=[graph.edata['feat'].to(device) for graph in graph_list] # 每天(图)里的边的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_graph: 90,\n",
      "len_feats: 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"len_graph: {len(graphs)},\\nlen_feats: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict所需要的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifLoadData_test=True\n",
    "graphPath_test=\"../data/graphs_test.pkl\"\n",
    "dataNodePath_test=\"../data/pre_node_test_4_A.csv\"\n",
    "dataEdgePath_test=\"../data/pre_edge_test_4_A.csv\"\n",
    "isTrain_test=False\n",
    "\n",
    "graph_list_test  = build_graph(ifLoadData_test, graphPath_test, dataNodePath_test, dataEdgePath_test,isTrain_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_test = copy.deepcopy(graphs)\n",
    "graphs_test.extend([graph.to(device) for graph in graph_list_test]) # 按天将图图取出来，就是src和tgt\n",
    "\n",
    "features_test = copy.deepcopy(features) \n",
    "features_test.extend([graph.ndata['feat'].to(device) for graph in graph_list_test]) # 每天(图)点的特征\n",
    "\n",
    "edge_features_test = copy.deepcopy(edge_features)\n",
    "edge_features_test.extend([graph.edata['feat'].to(device) for graph in graph_list_test]) # 每天(图)里的边的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拉取label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_label: 90\n",
      "[tensor([69.3060, 67.5410, 63.1170,  ..., 65.4980, 73.1520, 72.7780],\n",
      "       device='cuda:0'), tensor([68.8810, 65.3430, 60.3830,  ..., 64.0170, 72.9140, 70.4900],\n",
      "       device='cuda:0'), tensor([69.7380, 63.0020, 61.3210,  ..., 64.8800, 72.3910, 75.8860],\n",
      "       device='cuda:0'), tensor([68.7210, 65.7030, 60.8200,  ..., 66.1420, 73.7120, 72.0960],\n",
      "       device='cuda:0'), tensor([69.9600, 67.0040, 62.9880,  ..., 66.2720, 72.3940, 72.1170],\n",
      "       device='cuda:0'), tensor([68.1230, 66.2740, 61.7690,  ..., 65.8640, 72.7230, 70.9290],\n",
      "       device='cuda:0'), tensor([70.6340, 69.3050, 63.6920,  ..., 67.4300, 72.9020, 72.4490],\n",
      "       device='cuda:0'), tensor([70.5030, 65.8700, 62.1060,  ..., 66.3270, 72.2050, 73.2050],\n",
      "       device='cuda:0'), tensor([69.8910, 67.8490, 61.5820,  ..., 67.6790, 73.6380, 72.4870],\n",
      "       device='cuda:0'), tensor([69.2450, 66.6190, 59.4940,  ..., 66.1200, 72.7610, 72.1920],\n",
      "       device='cuda:0'), tensor([70.5150, 75.6140, 64.4090,  ..., 64.7040, 72.8500, 78.0770],\n",
      "       device='cuda:0'), tensor([69.8940, 65.9290, 62.8730,  ..., 65.5930, 73.4150, 75.1050],\n",
      "       device='cuda:0'), tensor([69.2210, 64.1500, 61.9460,  ..., 64.5960, 71.6270, 74.2580],\n",
      "       device='cuda:0'), tensor([70.0200, 65.4600, 63.4590,  ..., 66.8230, 71.8770, 73.0440],\n",
      "       device='cuda:0'), tensor([73.0760, 65.9540, 62.1340,  ..., 68.1770, 72.3570, 73.9440],\n",
      "       device='cuda:0'), tensor([68.6970, 64.8200, 60.9130,  ..., 69.2650, 73.1370, 72.8980],\n",
      "       device='cuda:0'), tensor([69.1580, 67.5380, 64.2930,  ..., 66.4530, 74.0150, 74.9510],\n",
      "       device='cuda:0'), tensor([68.2760, 65.1560, 64.1500,  ..., 64.3100, 73.1140, 74.1110],\n",
      "       device='cuda:0'), tensor([69.9180, 66.4260, 57.6600,  ..., 66.3380, 72.0550, 75.7390],\n",
      "       device='cuda:0'), tensor([68.5310, 65.6760, 59.5230,  ..., 66.0910, 72.0500, 73.5990],\n",
      "       device='cuda:0'), tensor([71.2030, 65.5050, 61.1400,  ..., 67.2750, 71.4520, 73.6460],\n",
      "       device='cuda:0'), tensor([69.8330, 65.6840, 60.9730,  ..., 68.2010, 72.5950, 73.7420],\n",
      "       device='cuda:0'), tensor([70.5320, 65.9600, 64.2260,  ..., 67.2950, 71.6650, 74.3360],\n",
      "       device='cuda:0'), tensor([69.6200, 65.9310, 61.8330,  ..., 66.1720, 72.6810, 74.5990],\n",
      "       device='cuda:0'), tensor([69.7430, 66.9500, 58.8740,  ..., 65.3940, 72.7700, 74.1410],\n",
      "       device='cuda:0'), tensor([68.1310, 65.2680, 62.2050,  ..., 67.2270, 72.3740, 75.0150],\n",
      "       device='cuda:0'), tensor([67.8000, 63.4340, 60.5370,  ..., 65.3920, 71.2400, 73.7280],\n",
      "       device='cuda:0'), tensor([68.4220, 65.7500, 62.2580,  ..., 65.5310, 71.7750, 73.2560],\n",
      "       device='cuda:0'), tensor([68.9370, 66.7960, 59.7340,  ..., 66.2260, 72.1850, 74.9980],\n",
      "       device='cuda:0'), tensor([72.3560, 64.1950, 60.1950,  ..., 66.8880, 72.2800, 74.8980],\n",
      "       device='cuda:0'), tensor([70.1820, 66.0480, 59.7210,  ..., 67.3600, 71.9890, 74.8430],\n",
      "       device='cuda:0'), tensor([68.9250, 65.0780, 65.7820,  ..., 67.2010, 72.4510, 73.9770],\n",
      "       device='cuda:0'), tensor([69.4450, 65.8930, 60.1860,  ..., 65.9820, 71.8180, 75.4160],\n",
      "       device='cuda:0'), tensor([68.4930, 67.4870, 62.7480,  ..., 64.3570, 70.9380, 73.3110],\n",
      "       device='cuda:0'), tensor([69.0080, 65.7090, 62.3270,  ..., 69.1860, 70.8960, 72.6990],\n",
      "       device='cuda:0'), tensor([70.5220, 65.2730, 64.2790,  ..., 66.0040, 73.0550, 74.4670],\n",
      "       device='cuda:0'), tensor([70.0950, 66.8670, 64.1120,  ..., 65.0190, 71.9930, 73.6600],\n",
      "       device='cuda:0'), tensor([68.6830, 68.6510, 67.8370,  ..., 67.7940, 72.0970, 74.4010],\n",
      "       device='cuda:0'), tensor([68.9690, 69.7740, 65.9680,  ..., 65.6900, 72.6180, 73.4580],\n",
      "       device='cuda:0'), tensor([69.0360, 68.3410, 67.1700,  ..., 63.8540, 74.5590, 72.6150],\n",
      "       device='cuda:0'), tensor([71.4590, 68.9630, 63.9490,  ..., 68.1010, 76.6030, 73.0220],\n",
      "       device='cuda:0'), tensor([70.1620, 67.9090, 64.6270,  ..., 65.1840, 71.9700, 74.3780],\n",
      "       device='cuda:0'), tensor([68.6430, 67.2230, 66.2180,  ..., 69.2000, 72.6330, 72.9610],\n",
      "       device='cuda:0'), tensor([72.1720, 67.1680, 66.4770,  ..., 68.5320, 72.9940, 73.1040],\n",
      "       device='cuda:0'), tensor([71.4510, 65.5480, 62.0050,  ..., 67.3970, 73.1400, 73.8560],\n",
      "       device='cuda:0'), tensor([70.3600, 71.0360, 65.4930,  ..., 69.1000, 74.3360, 73.3640],\n",
      "       device='cuda:0'), tensor([70.3150, 66.6620, 73.9320,  ..., 65.9610, 73.9780, 74.2710],\n",
      "       device='cuda:0'), tensor([71.0160, 68.2380, 67.6900,  ..., 65.2450, 74.6160, 72.9320],\n",
      "       device='cuda:0'), tensor([68.0360, 64.1370, 60.8520,  ..., 67.5970, 72.7900, 73.1430],\n",
      "       device='cuda:0'), tensor([70.7960, 66.0180, 77.9290,  ..., 67.7040, 74.1200, 74.2200],\n",
      "       device='cuda:0'), tensor([69.6960, 70.7560, 68.4040,  ..., 68.9390, 72.4280, 73.4090],\n",
      "       device='cuda:0'), tensor([69.3320, 65.6210, 66.0600,  ..., 66.9970, 73.2530, 73.5880],\n",
      "       device='cuda:0'), tensor([68.3710, 66.0910, 64.3970,  ..., 67.7300, 75.1040, 74.2510],\n",
      "       device='cuda:0'), tensor([69.0440, 65.0630, 64.1460,  ..., 65.2390, 72.8130, 73.7880],\n",
      "       device='cuda:0'), tensor([67.5750, 67.3550, 64.6330,  ..., 67.0340, 72.4660, 74.2830],\n",
      "       device='cuda:0'), tensor([68.9810, 65.2290, 65.1230,  ..., 65.5750, 72.3520, 73.0080],\n",
      "       device='cuda:0'), tensor([69.9390, 65.7460, 64.8090,  ..., 65.3730, 71.7250, 73.3000],\n",
      "       device='cuda:0'), tensor([69.6650, 65.8320, 65.7580,  ..., 67.9310, 71.9450, 74.4980],\n",
      "       device='cuda:0'), tensor([68.5750, 66.9390, 64.8020,  ..., 67.1850, 74.2730, 76.1060],\n",
      "       device='cuda:0'), tensor([69.1660, 66.9400, 65.1770,  ..., 65.2500, 77.6250, 72.6300],\n",
      "       device='cuda:0'), tensor([69.5120, 66.2550, 64.9260,  ..., 64.6910, 73.7420, 73.4980],\n",
      "       device='cuda:0'), tensor([68.3570, 65.9790, 65.4070,  ..., 66.4660, 72.3340, 72.2020],\n",
      "       device='cuda:0'), tensor([69.0980, 66.1310, 64.0390,  ..., 69.6330, 72.2840, 72.4230],\n",
      "       device='cuda:0'), tensor([77.4650, 66.3750, 63.9750,  ..., 69.4480, 73.6260, 72.3920],\n",
      "       device='cuda:0'), tensor([68.3510, 65.0510, 63.7830,  ..., 68.5510, 73.5960, 73.1770],\n",
      "       device='cuda:0'), tensor([68.5710, 63.5480, 68.1300,  ..., 68.0730, 74.8300, 71.9780],\n",
      "       device='cuda:0'), tensor([68.9210, 70.2120, 64.8420,  ..., 76.2410, 73.3590, 73.9880],\n",
      "       device='cuda:0'), tensor([68.7950, 66.4200, 64.1360,  ..., 65.8180, 74.3670, 72.4510],\n",
      "       device='cuda:0'), tensor([70.3600, 64.8830, 67.1350,  ..., 67.1720, 71.9190, 74.2680],\n",
      "       device='cuda:0'), tensor([70.3530, 66.6850, 65.3700,  ..., 67.2950, 73.1500, 72.5230],\n",
      "       device='cuda:0'), tensor([68.5080, 66.8390, 64.3860,  ..., 67.0630, 72.5260, 73.1820],\n",
      "       device='cuda:0'), tensor([69.3900, 66.0820, 63.6200,  ..., 66.6210, 72.0590, 72.6330],\n",
      "       device='cuda:0'), tensor([70.7540, 67.7430, 64.7370,  ..., 65.4420, 72.0160, 74.2390],\n",
      "       device='cuda:0'), tensor([72.2680, 66.9150, 63.7930,  ..., 62.9800, 73.0980, 74.2970],\n",
      "       device='cuda:0'), tensor([69.8610, 64.4850, 66.1070,  ..., 66.7460, 74.4450, 75.3060],\n",
      "       device='cuda:0'), tensor([68.8950, 64.6850, 67.1210,  ..., 72.8130, 72.5220, 74.2770],\n",
      "       device='cuda:0'), tensor([70.9480, 66.9920, 65.0250,  ..., 68.4570, 72.6180, 73.8260],\n",
      "       device='cuda:0'), tensor([71.1700, 63.9600, 70.5560,  ..., 70.6070, 73.9200, 75.2140],\n",
      "       device='cuda:0'), tensor([70.0880, 64.0940, 67.6230,  ..., 71.8410, 75.1500, 75.7830],\n",
      "       device='cuda:0'), tensor([70.9590, 66.2770, 69.5590,  ..., 71.5600, 75.3460, 75.5610],\n",
      "       device='cuda:0'), tensor([70.4200, 74.2860, 67.4170,  ..., 69.6090, 75.7760, 75.6090],\n",
      "       device='cuda:0'), tensor([69.7270, 64.6930, 76.1970,  ..., 70.9690, 74.0490, 74.8030],\n",
      "       device='cuda:0'), tensor([68.7620, 61.8330, 63.4200,  ..., 69.3940, 74.6520, 75.2590],\n",
      "       device='cuda:0'), tensor([70.0620, 64.3340, 69.3250,  ..., 70.7590, 74.5320, 74.3580],\n",
      "       device='cuda:0'), tensor([69.2490, 60.6740, 67.5630,  ..., 81.6640, 78.1520, 76.4570],\n",
      "       device='cuda:0'), tensor([69.5790, 63.4140, 66.8970,  ..., 70.6260, 73.6210, 74.5820],\n",
      "       device='cuda:0'), tensor([65.4000, 64.4980, 67.2100,  ..., 70.0620, 71.5840, 73.1940],\n",
      "       device='cuda:0'), tensor([67.6180, 61.3920, 65.9540,  ..., 68.9020, 70.1790, 72.7130],\n",
      "       device='cuda:0'), tensor([66.9000, 63.7020, 62.3540,  ..., 67.0340, 70.7270, 72.3940],\n",
      "       device='cuda:0'), tensor([66.5690, 65.3780, 62.1190,  ..., 69.8570, 71.0390, 71.6460],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "active_index_train, consume_index_train, active_index_test, consume_index_test = [], [], [], []\n",
    "\n",
    "for graph in graphs:\n",
    "    active_index_train.append(graph.ndata['active'])\n",
    "    consume_index_train.append(graph.ndata['consume'])\n",
    "\n",
    "# [90,1140,2]\n",
    "print(f\"len_label: {len(active_index_train)}\")\n",
    "print(active_index_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_index_train = torch.stack(active_index_train)\n",
    "consume_index_train = torch.stack(consume_index_train)\n",
    "target_train = torch.cat([active_index_train.unsqueeze(2), consume_index_train.unsqueeze(2)], dim=-1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 1140, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGELayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, device):\n",
    "        super(GraphSAGELayer, self).__init__()\n",
    "        # self.sage = SAGEConv(in_feats, out_feats, 'mean').to(device)\n",
    "        self.sage = GraphConv(in_feats, out_feats,allow_zero_in_degree=True).to(device)\n",
    "        self.norm = nn.BatchNorm1d(out_feats).to(device)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.sage(g, features)\n",
    "        return (F.relu(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGELSTMModel(nn.Module):\n",
    "    def __init__(self, in_feats, sage_hidden_feats, lstm_hidden_size, mlp_hidden_size, num_layers, device):\n",
    "        super().__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.graphsage = GraphSAGELayer(in_feats, sage_hidden_feats, device)\n",
    "        self.lstm = nn.LSTM(in_feats, lstm_hidden_size, num_layers).to(device)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(sage_hidden_feats + lstm_hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 2)\n",
    "        ).to(device)\n",
    "        self.norm = nn.BatchNorm1d(self.lstm_hidden_size).to(device)\n",
    "\n",
    "    def forward(self, graphs, feats):\n",
    "        # GraphSAGE 处理图数据\n",
    "        # sage_out维度[num_graph, num_node, features]=[90,1140,hidden_sage]\n",
    "        sage_out = [self.graphsage(g, feature) for g, feature in zip(graphs, feats)] \n",
    "        sage_out = torch.stack(sage_out)\n",
    "        \n",
    "        # sage_out = torch.stack(sage_out1, dim=1)  # 维度：[node_num, Time_step, hidden_feats]\n",
    "\n",
    "        features=torch.stack(feats,dim=0)\n",
    "\n",
    "        # h0,c0形状[num_graph/time, num_node, hidden_lstm]\n",
    "        h0=Variable(torch.zeros(self.num_layers,features.shape[1],self.lstm_hidden_size)).to(device)\n",
    "        c0=Variable(torch.zeros(self.num_layers,features.shape[1],self.lstm_hidden_size)).to(device)\n",
    "        lstm_out, (hn,cn) = self.lstm(features,(h0,c0))\n",
    "        # lstm_out_reshaped = lstm_out.reshape(-1, self.lstm_hidden_size)\n",
    "        \n",
    "        # [num_graph/time, num_node, hidden_lstm+hidden_sage]\n",
    "\n",
    "        cat_sage_lstm=torch.cat((sage_out,lstm_out),dim=2) \n",
    "        # lstm_out = lstm_out_result.reshape(lstm_out.shape[0],lstm_out.shape[1], lstm_out.shape[2])\n",
    "\n",
    "        # 维度[num_graph/time, num_node, 2]\n",
    "        act_cos = self.fc(cat_sage_lstm)\n",
    "\n",
    "        return act_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.MSELoss()\n",
    "\n",
    "model=GraphSAGELSTMModel(\n",
    "    in_feats=35,\n",
    "    sage_hidden_feats=32,\n",
    "    lstm_hidden_size=32,\n",
    "    mlp_hidden_size=128,\n",
    "    num_layers=1,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "epochs=100\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 100/100 [00:20<00:00,  4.81it/s, loss=3.79]\n"
     ]
    }
   ],
   "source": [
    "tqdm_iter = tqdm(range(epochs), desc='Epochs')\n",
    "for epoch in tqdm_iter:\n",
    "    train_loss_list = []\n",
    "    model.train()\n",
    "    predict = model(graphs, features).cpu()\n",
    "    train_loss = torch.sqrt(loss_fn(predict, target_train))  # [Time_step*nodes_num, 2]\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_list.append(train_loss.cpu().detach().numpy())\n",
    "\n",
    "    # 使用 set_postfix 更新进度条后缀\n",
    "    tqdm_iter.set_postfix({'loss': train_loss.item()}, refresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7592, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predict = model(graphs_test, features_test).cpu()\n",
    "train_loss = torch.sqrt(loss_fn(predict[:90], target_train))  # [Time_step*nodes_num, 2]\n",
    "\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=torch.transpose(predict[-4:],0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1140, 4, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array=predict_test.detach().numpy().reshape((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other_info = pd.read_csv(\"../data/A/node_test_4_A.csv\")\n",
    "\n",
    "id=df_other_info[\"geohash_id\"]\n",
    "date=df_other_info[\"date_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({\n",
    "    \"geohash_id\": id,\n",
    "    \"consumption_level\":predict_array[:, 1],\n",
    "    \"activity_level\":predict_array[:, 0],\n",
    "    \"date_id\": date\n",
    "})\n",
    "\n",
    "# 将合并后的DataFrame保存为新的CSV文件\n",
    "df_result.to_csv(\"reslut-3G.csv\",sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geshin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
